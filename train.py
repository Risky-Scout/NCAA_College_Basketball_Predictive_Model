#!/usr/bin/env python3
"""
================================================================================
NCAA College Basketball Predictive Model - Training Script
================================================================================

This script is run daily by GitHub Actions at 7:00 AM EST.

It:
1. Loads historical game data
2. Engineers features with leakage protection
3. Applies recency weighting
4. Trains the meta-ensemble
5. Validates on holdout set
6. Saves the updated model

================================================================================
"""

import numpy as np
import pandas as pd
from datetime import datetime, timedelta
from pathlib import Path
import json
import sys

# Add src to path
sys.path.insert(0, str(Path(__file__).parent))

from config import model_config, DATA_DIR, MODELS_DIR
from data_pipeline import TeamDatabase, HistoricalDataManager
from feature_engineering import FeatureEngineer
from meta_ensemble import MetaEnsemble


def generate_training_data(n_games: int = 5000) -> tuple:
    """
    Generate training data.
    
    In production, this would load from historical database.
    For now, generates high-quality synthetic data.
    
    CRITICAL: All features are computed with proper temporal ordering
    to prevent data leakage.
    """
    print("\nGenerating training data...")
    
    np.random.seed(42)
    
    team_db = TeamDatabase()
    feature_eng = FeatureEngineer()
    
    X, y_win, y_margin, y_total = [], [], [], []
    dates = []  # For temporal ordering
    
    # Generate games with realistic distributions
    teams = list(team_db.teams.keys())
    
    for i in range(n_games):
        # Random matchup
        home_team = np.random.choice(teams)
        away_team = np.random.choice([t for t in teams if t != home_team])
        
        home_stats = team_db.get(home_team)
        away_stats = team_db.get(away_team)
        
        if not home_stats or not away_stats:
            continue
        
        # Generate game date (for temporal ordering)
        days_ago = np.random.randint(0, 365 * 3)  # 3 seasons
        game_date = datetime.now() - timedelta(days=days_ago)
        dates.append(game_date)
        
        # Situational context
        home_rest = np.random.choice([1, 2, 3, 4, 5, 6, 7], p=[0.05, 0.15, 0.35, 0.25, 0.10, 0.05, 0.05])
        away_rest = np.random.choice([1, 2, 3, 4, 5, 6, 7], p=[0.05, 0.15, 0.35, 0.25, 0.10, 0.05, 0.05])
        neutral_site = np.random.choice([0, 1], p=[0.92, 0.08])
        conf_game = np.random.choice([0, 1], p=[0.4, 0.6])
        
        context = {
            'home_rest': home_rest,
            'away_rest': away_rest,
            'neutral_site': neutral_site,
            'conference_game': conf_game,
            'rivalry': np.random.choice([0, 1], p=[0.90, 0.10]),
            'away_travel_miles': np.random.uniform(100, 2000),
        }
        
        # True margin (what we're trying to predict)
        hca = 3.5 * (1 - neutral_site)
        rest_adj = (home_rest - away_rest) * 0.3
        true_margin = (home_stats['adj_em'] - away_stats['adj_em']) + hca + rest_adj
        
        # Market simulation (imperfect estimate of true margin)
        market_spread = true_margin + np.random.normal(0, 2)
        
        market = {
            'spread': round(market_spread * 2) / 2,
            'total': 145 + np.random.normal(0, 5),
            'opening_spread': market_spread + np.random.normal(0, 1.5),
            'line_movement': np.random.normal(0, 1),
            'steam_move': np.random.choice([0, 1], p=[0.92, 0.08]),
            'public_pct': np.clip(50 + (market_spread * 2) + np.random.normal(0, 10), 20, 80),
        }
        
        # Create features
        features = feature_eng.create_features(home_stats, away_stats, context, market)
        feature_vec = [features.get(f, 0) for f in feature_eng.feature_names]
        X.append(feature_vec)
        
        # Generate outcomes with realistic variance
        actual_margin = true_margin + np.random.normal(0, 11)
        expected_total = 145 + (home_stats['adj_t'] + away_stats['adj_t'] - 136) * 0.5
        actual_total = expected_total + np.random.normal(0, 12)
        
        y_margin.append(actual_margin)
        y_total.append(actual_total)
        y_win.append(1 if actual_margin > 0 else 0)
    
    # Sort by date (temporal ordering for proper CV)
    sort_idx = np.argsort(dates)
    X = np.array(X)[sort_idx]
    y_win = np.array(y_win)[sort_idx]
    y_margin = np.array(y_margin)[sort_idx]
    y_total = np.array(y_total)[sort_idx]
    
    print(f"  Generated {len(y_win)} games")
    print(f"  Home win rate: {y_win.mean():.1%}")
    print(f"  Mean margin: {y_margin.mean():.1f}")
    print(f"  Mean total: {y_total.mean():.1f}")
    
    return X, y_win, y_margin, y_total, feature_eng.feature_names


def calculate_recency_weights(n_samples: int, decay: float = 2.0) -> np.ndarray:
    """
    Calculate exponential recency weights.
    
    More recent games get higher weight in training.
    
    Args:
        n_samples: Number of samples
        decay: Decay factor (higher = more weight on recent)
    
    Returns:
        Normalized weights array
    """
    weights = np.exp(np.linspace(-decay, 0, n_samples))
    weights /= weights.sum()  # Normalize
    weights *= n_samples  # Scale back up
    return weights


def train():
    """Main training function"""
    print("=" * 60)
    print("NCAA CBB MODEL - DAILY TRAINING")
    print("=" * 60)
    print(f"Timestamp: {datetime.now().isoformat()}")
    
    # Generate training data
    X, y_win, y_margin, y_total, feature_names = generate_training_data(n_games=5000)
    
    # Calculate recency weights
    print("\nCalculating recency weights...")
    weights = calculate_recency_weights(len(y_win), decay=model_config.recency_decay)
    print(f"  Oldest sample weight: {weights[0]:.4f}")
    print(f"  Newest sample weight: {weights[-1]:.4f}")
    
    # Train model
    model = MetaEnsemble(config=model_config)
    metrics = model.train(
        X, y_win, y_margin, y_total,
        sample_weights=weights,
        feature_names=feature_names
    )
    
    # Print top features
    print("\nTop 10 Features by Importance:")
    for name, importance in model.get_top_features(10):
        print(f"  {name}: {importance:.4f}")
    
    # Save model
    model.save()
    
    # Save training metrics
    metrics_path = DATA_DIR / "training_metrics.json"
    with open(metrics_path, 'w') as f:
        json.dump(metrics, f, indent=2)
    print(f"\nâœ“ Training metrics saved to {metrics_path}")
    
    return model, metrics


if __name__ == "__main__":
    model, metrics = train()
